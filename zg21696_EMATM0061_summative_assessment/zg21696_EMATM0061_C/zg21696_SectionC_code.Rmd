---
title: "zg21696_SectionC"
author: "Ruinan Wang"
date: "13/12/2021"
output: html_document
---
K-Nearest Neighbour Algorithms is easy to understand for machine learning freshers. However, it contains many problems which deserve to study and learn. This section will introduce how KNN works, explore some performance and efficiency problems when implementing KNN algorithm in an actual case and corresponding optimizations. Then KNN applying for KD tree to search neighbours will be introduced and this method will compare with traditional KNN on searching time and performance.  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(dplyr)
```
## 3.1 Introduction for K-Nearest Neighbour algorithms
K-Nearest Neighbour is abbreviated as KNN, which can be used to solve both regression and classification problems. The core concept of KNN can be summarised as that if things are similar, they are closed to each other. Briefly speaking, in KNN, the class or the predicting value of test samples is determined by the “votes” of the K nearest neighbours. The following is the specific steps of using basic KNN.  
1. Map the training data to n dimension space. (n is the number of features in a dataset)  
2. Choose a K as the number of neighbours (if classification, K should be an odd number in most cases)  
3. Computing the distance between the test data and each training data according to the feature value.  
4. Ordering the distances from smallest to largest (ascending order)  
5. Retrieve the first K in this order  
6. If regression, the mean of the targeted labels of these K neighbours need to be calculated. If classification, the mode of the classes of these K neighbours need to be calculated.  
  
KNN is suitable to deal with the dataset with a small distance between classes, but it is vulnerable for unbalanced datasets and meanwhile, time complexity will become very large if the amount of training dataset is enormous. The following will describe in more detail and give some optimization.  

## 3.2 Importing Data set and pretreatment  
The dataset is obtained from UCL Machine Learning repository about breast cancer patients in Wisconsin with malignant and benign tumours (Wolberg, Street, Mangasarian, 1995). The creators are W.H.Wolberg, W.N.Street and O.L.Mangasarian.[1]  

```{r}
breast_cancer_ds = read.csv("data.csv", header = TRUE)
```
View the data dimension. This dataset has 569 samples and 32 features and doesn't have the missing value.
```{r}
sum(is.na(breast_cancer_ds))
dim(breast_cancer_ds)
```
View the data structure. The column "id" doesn't affect the prediction, so it needs to be deleted.  
The column "diagnosis" is a categorical variable, and all other feature types are continuous.  
  
Meanwhile, the column "diagnosis" is a dependent variable, but its attribute type is a character and needs to be converted to the numerical value where B = 1 and M = 0.  

```{r}
str(breast_cancer_ds)
breast_cancer_ds = breast_cancer_ds %>% select(-c(id))
breast_cancer_ds$diagnosis[which(breast_cancer_ds$diagnosis == 'M')] = 0
breast_cancer_ds$diagnosis[which(breast_cancer_ds$diagnosis == 'B')] = 1
breast_cancer_ds$diagnosis = factor(breast_cancer_ds$diagnosis)
```
  
  
But, the magnitude for some features have a significant gap, like "radius_mean" and "area_mean". If the scaling to features in the dataset isn't implemented, the result of the classifier will heavily bias towards features with a greater magnitude and ignore the influence of the other features. And distance-based algorithms like KNN are the most vulnerable to unscaled data.  
```{r}
head(select(breast_cancer_ds,symmetry_mean,area_mean),3)
```
For avoiding such a consequence, feature scaling should be executed. Z-score standardization, $X^\star (after\space scaling)= \frac{X-\overline X}{std}$ will be applied in the following exercise. Using standardization, in each dimension in datasets, mean value will be 0 and variance will be 1.
```{r}
standardization <- function (x){
  return((x-mean(x))/(sd(x)))
}
bc_df <- as.data.frame(lapply(breast_cancer_ds[2:31],standardization))
breast_cancer_ds <- cbind(breast_cancer_ds[1],bc_df)
```

Finally, the data split will be be done. The ratio of train-validation data to test data is 3:1.
```{r}
num_total <- breast_cancer_ds %>% nrow()
num_test <- ceiling(0.25*num_total)
set.seed(1)
breast_cancer_ds <- breast_cancer_ds %>% sample_n(size=nrow(.))
test_inds <- seq(num_total-num_test+1,num_total)
test_data <- breast_cancer_ds %>% filter (row_number()%in%test_inds)
train_validation_data <- breast_cancer_ds %>% filter(!row_number() %in% test_inds)
```

## 3.3 Explore how the performance of model varies (both on train data and validation data) when the amount of training dataset changes.
Firstly, the explored problem is a binary classification and the dataset is not so unbalanced.
```{r}
prop.table(table(breast_cancer_ds$diagnosis))
```

So accuracy value is chosen as the metric for the performance of the model rather than Precision value or recall value. Accuracy=$TP+TN\over ALL$ 

Here is the confusion matrix.

|  | Positive | Negative |
| :----: | :----: | :----: |
| True | TP | TN |
| False | FP | FN |
TP means True Positive, the number of predicting correctly the tumour is malignant.  
TN means True Negative, the number of predicting correctly the tumour is benign.  
FP means False Positive, the number of predicting wrongly the tumour is malignant.  
FN means False Negative, the number of predicting wrongly the tumour is benign.
  
  
  
After defining the accuracy value as the appropriate metric to evaluate the classifier, the solution will be designed. The solution integrates the Crossing Validation idea to improve the reliability of results. let each fold of train_validation data set become validation dataset once.
  

Firstly, the train_validation dataset will be divided into some folds (I choose 5).  the validation dataset and the training dataset in each iteration will be taken as a list to store in a list variable (nested list). If it has 5 iterations, this variable will have 5 different validation datasets and 5 different training datasets.
```{r}
split_data_CV = function(data,folds){
  num = data%>%nrow()
  num_per_fold = ceiling(num/folds)
  splited_dataset = list()
  for (iter in 1:folds) {
    fold_start = (iter-1)*num_per_fold+1
    fold_end = min(iter*num_per_fold,num)
    fold_indices = seq(fold_start,fold_end)
    splited_dataset[[iter]] = list("train"= data %>% 
                                     filter(!row_number()%in%fold_indices), 
                                   "validation"= data %>%
                                     filter(row_number()%in%fold_indices)) 
  }
  return(splited_dataset)
}
```


Create a function to reduce the amount of training dataset in each iteration and return this decreased training dataset. In addition, for improving sampling randomness further, the seed argument will be the iteration index.
```{r}
train_data_decrease = function(initial_train_data, wanted_train_data_num,fold){
  set.seed(fold)
  initial_data_num = initial_train_data%>% nrow()
  data_indices = sample(1:initial_data_num,wanted_train_data_num, replace = FALSE)
  wanted_train_data = initial_train_data %>% filter(row_number()%in%data_indices)
  return(wanted_train_data)
}
```

Each decreased training dataset and the validation dataset will be poured into the KNN classifier and the accuracy will be calculated. Meanwhile, if K is not declared, it will be 10 by default.
```{r}
library(kknn)
kknn_accuracy = function(ds1,ds2, k_neighbor=10){
  check = kknn(diagnosis~.,train = ds1, test = ds2, k=k_neighbor, distance = 2, kernel ="rectangular")
  fit = fitted(check)
  confusion_matrix = table(ds2$diagnosis, fit)
  acc =  sum(diag(confusion_matrix))/sum(confusion_matrix)
  return(acc)
}

```

In these 5 iterations, every time the train data amount decreases, the accuracy on the decreased training dataset and the validation dataset will be stored. Then take the average of the accuracy of the same amount of training dataset in iterations.    
Finally, plot how the average accuracy on the training dataset and validation dataset changes when the amount of the training dataset changes.
```{r}
##I set that every time the amount of data decreases, I will remove three pieces of data, and the K-fold is 5
folds = 5
inc = 3
all_sample_list = split_data_CV (train_validation_data,folds)
train_total_num = floor(train_validation_data%>%nrow()*(folds-1)/folds)
acc_by_train_ds_num = crossing(sample_list_index = seq(1,5), 
                        train_data_num = seq(15,train_total_num,inc))%>%
  mutate(train = pmap(.l = list(sample_list_index,train_data_num),
                     .f = ~ train_data_decrease(all_sample_list[[.x]][["train"]],
                                                     .y,.x)))%>%
  mutate(validation = map(.x=sample_list_index,.f=~all_sample_list[[.x]][["validation"]]))%>%
  mutate(acc_validation = map2_dbl(.x = train,.y = validation, .f = ~kknn_accuracy(ds1 =.x,ds2=.y)))%>%
  mutate(acc_train = map_dbl(.x = train,.f = ~kknn_accuracy(ds1=.x,ds2=.x)))%>%
  group_by(train_data_num)%>%
  summarize(validation_accuracy = mean(acc_validation), train_accuracy = mean(acc_train))

acc_by_train_ds_num <- acc_by_train_ds_num%>%
  pivot_longer(cols=c(validation_accuracy,train_accuracy),names_to = "Type", values_to = "accuracy")
```
It can be seen from the figure that both the accuracy on the training dataset and the validation dataset surge before the training data increases to about 100, however, the growth rate of both slows down later.
```{r}
library(ggplot2)
ggplot(data=acc_by_train_ds_num,aes(x=train_data_num,y=accuracy,color =Type,linetype= Type))+ geom_smooth() + theme_bw() 
```

## 3.4 Explore how the performance of model varies (both on train data and validation data) when the hyperparameter, k,changes
Similarly, in this part, the idea for crossing validation is still used to get the more reliable accuracy changing situation as the hyperparameter k changes.
```{r}
folds = 5
all_folds_ds = split_data_CV (train_validation_data,folds)
acc_by_hyperparameter = crossing(k = seq(1,100,5),
                                 fold_index = 1:folds)%>%
  mutate(train_per_fold = map(.x=fold_index, .f=~all_folds_ds[[.x]][["train"]]))%>%
  mutate(validation_per_fold = map(.x=fold_index, .f=~all_folds_ds[[.x]][["validation"]]))%>%
  mutate(acc_train = map2_dbl(.x = train_per_fold, .y=k, .f=~kknn_accuracy(ds1=.x,ds2=.x,k_neighbor =.y)))%>%
  mutate(acc_validation = pmap_dbl(.l= list(train_per_fold, validation_per_fold,k),.f=~kknn_accuracy(ds1=..1,ds2=..2,k_neighbor =..3)))%>%
  group_by(k)%>%
  summarize(validation_accuracy = mean(acc_validation), train_accuracy = mean(acc_train))

acc_by_hyperparameter <- acc_by_hyperparameter%>%
  pivot_longer(cols=c(validation_accuracy,train_accuracy),names_to = "Type", values_to = "accuracy")

```
This chart points that, the accuracy on the training dataset decreases continuously as the k-neighbours increases. On the contrary, the accuracy on the validation dataset rises to the peak when the k-neighbours is approximately 25 and then decreases constantly.
```{r}
ggplot(data=acc_by_hyperparameter,aes(x=k,y=accuracy,color =Type,linetype= Type))+geom_smooth() + theme_bw() 

```

## 3.5 Hyperparameter Tuning through RandomizedSearchCV
In KNN, k-neighbours is not only the hyperparameter, but the way to compute the distance and the function to add weights.  
    
    
Generally speaking, Euclidean distance $\sqrt{\sum_{r=1}^n{(x_i-y_i)^2}}$is selected as the distance metric by default. But there are many other distance measures, like cosine distance, Mahalanobis distance. Each specific dataset has a different optimal distance measurement for classification or regression. The following will only explore the generalization of Euclidean distance, Minkowski distance $\left(\sum_{i=1}^n|x_i-y_i|^p\right)^{1/p}$ as distance metric where p is a hyperparameter.  
  
  
Real-world datasets have different data volumes for different classes, leading to a situation where the result just belongs to the class with more data volume. This problem especially influences the model performance for datasets with a small distance between classes.That is why weighting the first k neighbours is important. KKNN package provides many kernel functions to add weights,like Gaussian kernel $W(x)=ae^{-{(x-b)^2\over 2c^2}}\space(a,b,c\in R)$,  




remotes::install_github('mlampros/RandomSearchR')
In this part, an external package named RandomSearchR by MOUSELIMIS is used for implementing RandomizedSearchCV to tune hyperparameter.
```{r, results='hide', warning=FALSE}
library(RandomSearchR)
## list all choices of hyperparameter
grid_kknn = list(k = 1:50, 
                 distance = 1:5,
                 kernel = c("rectangular", "triangular", "epanechnikov", "biweight","cos", "inv", "gaussian", "rank", "optimal"))

## make diagnosis results become numerical value (1,2)
diagnosis_vec = train_validation_data[,1]
diagnosis_vec = c(1:length(unique(diagnosis_vec)))[match(diagnosis_vec, sort(unique(diagnosis_vec)))]
data_vec = train_validation_data[,-1]
## create a formula for the following random_search_resample function
form <- as.formula(paste('diagnosis ~', paste(names(data_vec),collapse = '+')))

ALL_DATA = train_validation_data
## execute the RandomizedSearchCV of which the number of randomly sampling hyperparameter combinations (iteration) is 30 and K-folds for crossing validation is 5.
res_knn = random_search_resample( as.factor(diagnosis_vec),
                                  tune_iters = 50,
                                  resampling_method = list(method = 'cross_validation',
                                                           repeats=NULL, 
                                                           sample_rate = NULL,
                                                           folds = 5),
                                  ALGORITHM = list(package = require(kknn), algorithm = kknn),
                                  grid_params = grid_kknn,
                                  DATA = list(formula = form, train = ALL_DATA),
                                  Args = NULL,
                                  regression = FALSE,
                                  re_run_params=FALSE)
```
### 
```{r}
##computing accuracy
acc = function(y_true, preds) { 
  out = table(y_true, max.col(preds, ties.method = "random"))
  acc = sum(diag(out))/sum(out)
  acc
}

##Obtaining the evaluation result for performance of this KNN classifier
perf = performance_measures(list_objects = list(kknn = res_knn),
                            eval_metric = acc,
                            sort = list(variable = 'Mean', decreasing = TRUE))
accuracy_validation_table = dplyr:: select(perf$test_params$kknn,k,distance,kernel,accuracy=Mean) 
accuracy_train_table = dplyr:: select(perf$train_params$kknn, k,distance,kernel,accuracy=Mean)
head(accuracy_validation_table,10)

```


The table above is the accuracy rank list of the model with different hyperparameters on the validation dataset after RandomizedSearchCV, so the optimal hyperparameters should be k=11, p_distance (Minkowski) = 3, kernel function="epanechnikov".


##report 

```{r}
library(caret)
test_knn = kknn(diagnosis~., train = train_validation_data, test = test_data, k = 11, distance = 3, kernel = "epanechnikov")
truth = test_data$diagnosis
pred = fitted(test_knn)
confusionMatrix(table(pred, truth))
```

## the introduction of kd tree
```{r}
library(RANN)
knn_on_kdtree = system.time(lapply(1:1000,function(x) test1 <- nn2(train_validation_data[-1], k=11, query = test_data[-1])))[3]
normal_knn = system.time(lapply(1:1000,function(x) test2 <-  kknn(diagnosis~., train_validation_data, test_data, k = 11)))[3]
time_table = data.frame(Type = c("KNN search on a KD Tree","Traditional KNN"), Time = c(as.numeric(knn_on_kdtree),as.numeric(normal_knn)))
time_table
ggplot(data = time_table, aes(x=Type,y=Time,fill=Type))+ 
  geom_bar(stat="identity",width=0.5)+
  labs(title = "The comparision about Time Elapsed", y="Time in Seconds") + 
  theme(plot.title = element_text(hjust = 0.5))
```


```{r}
compute_mode =  function(vec){
  temp = table(vec)
  nums_of_mode <- max(temp)
  mode = as.numeric(names(temp))[temp == nums_of_mode]
  return(mode)
}

kdtree_knn = function(train,test){
  neighbours_list =nn2(train[-1], k=11, query = test[-1])[["nn.idx"]]
  row_num = neighbours_list%>%nrow()
  preds = c()
  for(x in 1:row_num){
    index = neighbours_list[x,]
    neighbours = train%>%filter(row_number()%in%index)%>%select(diagnosis)
    preds = c(preds,compute_mode(neighbours))
  }
  return(preds)
}

traditional_knn = function(train,test){
  check_knn = kknn(diagnosis~., train, test, k = 11)
  pred_vector = fitted(check_knn)
  return(pred_vector)
}

preds_kdtree = kdtree_knn(train_validation_data,test_data)
preds_tradition = traditional_knn(train_validation_data,test_data)
truth = test_data$diagnosis
kd_preds_table = table(truth,preds_kdtree)
kd_preds_table
trad_preds_table = table(truth,preds_tradition)
trad_preds_table

accuracy = c("the accuracy of the KNN on a KD tree"= sum(diag(kd_preds_table))/sum(kd_preds_table), 
               "the accuracy of the traditional KNN" = sum(diag(trad_preds_table))/sum(trad_preds_table))
accuracy

```



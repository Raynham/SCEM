---
title: "zg21696_EMATM0061_C_Report"
author: "Ruinan Wang"
date: "13/12/2021"
output: html_document
---
K-Nearest Neighbour Algorithms is easy to understand for machine learning freshers. However, it contains many problems which deserve to study and learn. This section will introduce how KNN works, explore some performance and efficiency problems when implementing KNN algorithm in an actual case and corresponding optimizations. Then KNN applying for KD tree to search neighbours will be introduced and this method will compare with traditional KNN on searching time and performance.  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
```
## 3.1 Introduction for K-Nearest Neighbour algorithms
K-Nearest Neighbour is abbreviated as KNN, which can be used to solve both regression and classification problems. The core concept of KNN can be summarised as that if things are similar, they are closed to each other. Briefly speaking, in KNN, the class or the predicting value of test samples is determined by the “votes” of the K nearest neighbours. The following is the specific steps of using basic KNN.  
1. Map the training data to n dimension space. (n is the number of features in a dataset)  
2. Choose a K as the number of neighbours (if classification, K should be an odd number in most cases)  
3. Computing the distance between the test data and each training data according to the feature value.  
4. Ordering the distances from smallest to largest (ascending order)  
5. Retrieve the first K in this order  
6. If regression, the mean of the targeted labels of these K neighbours need to be calculated. If classification, the mode of the classes of these K neighbours need to be calculated.  
  
KNN is suitable to deal with the dataset with a small distance between classes, but it is vulnerable for unbalanced datasets and meanwhile, time complexity will become very large if the amount of training dataset is enormous. The following will describe in more detail and give some optimization.  

## 3.2 Importing Data set and pretreatment  
The dataset is obtained from UCL Machine Learning repository about breast cancer patients in Wisconsin with malignant and benign tumours (Wolberg, Street, Mangasarian, 1995).
```{r}
breast_cancer_ds = read.csv("data.csv", header = TRUE)
```
View the data dimension. This dataset has 569 samples and 32 features and doesn't have the missing value.
```{r}
sum(is.na(breast_cancer_ds))
dim(breast_cancer_ds)
```
View the data structure. The column "id" doesn't affect the prediction, so it needs to be deleted.  
The column "diagnosis" is a categorical variable, and all other feature types are continuous.  
  
Meanwhile, the column "diagnosis" is a dependent variable, but its attribute type is a character and needs to be converted to the numerical value where B = 1 and M = 0.  

```{r}
str(breast_cancer_ds)
breast_cancer_ds = breast_cancer_ds %>% select(-c(id))
breast_cancer_ds$diagnosis[which(breast_cancer_ds$diagnosis == 'M')] = 0
breast_cancer_ds$diagnosis[which(breast_cancer_ds$diagnosis == 'B')] = 1
breast_cancer_ds$diagnosis = factor(breast_cancer_ds$diagnosis)
```
  
  
But, the magnitude for some features have a significant gap, like "radius_mean" and "area_mean". If the scaling to features in the dataset isn't implemented, the result of the classifier will heavily bias towards features with a greater magnitude and ignore the influence of the other features. And distance-based algorithms like KNN are the most vulnerable to unscaled data.  
```{r}
head(select(breast_cancer_ds,symmetry_mean,area_mean),3)
```
For avoiding such a consequence, feature scaling should be executed. Z-score standardization, $X^\star (after\space scaling)= \frac{X-\overline X}{std}$ will be applied in the following exercise. Using standardization, in each dimension in datasets, mean value will be 0 and variance will be 1.
```{r}
standardization <- function (x){
  return((x-mean(x))/(sd(x)))
}
bc_df <- as.data.frame(lapply(breast_cancer_ds[2:31],standardization))
breast_cancer_ds <- cbind(breast_cancer_ds[1],bc_df)
```

Finally, the data split will be be done. The ratio of train-validation data to test data is 3:1.
```{r}
num_total <- breast_cancer_ds %>% nrow()
num_test <- ceiling(0.25*num_total)
set.seed(1)
breast_cancer_ds <- breast_cancer_ds %>% sample_n(size=nrow(.))
test_inds <- seq(num_total-num_test+1,num_total)
test_data <- breast_cancer_ds %>% filter (row_number()%in%test_inds)
train_validation_data <- breast_cancer_ds %>% filter(!row_number() %in% test_inds)
```

## 3.3 Explore how the performance of model varies (both on train data and validation data) when the amount of training dataset changes.
Firstly, the explored problem is a binary classification and the dataset is not so unbalanced.
```{r}
prop.table(table(breast_cancer_ds$diagnosis))
```

So accuracy value is chosen as the metric for the performance of the model rather than Precision value or recall value. Accuracy=$TP+TN\over ALL$ 

Here is the confusion matrix.

|  | Positive | Negative |
| :----: | :----: | :----: |
| True | TP | TN |
| False | FP | FN |
TP means True Positive, the number of predicting correctly the tumour is malignant.  
TN means True Negative, the number of predicting correctly the tumour is benign.  
FP means False Positive, the number of predicting wrongly the tumour is malignant.  
FN means False Negative, the number of predicting wrongly the tumour is benign.
  
  
  
After defining the accuracy value as the appropriate metric to evaluate the classifier, the solution will be designed. The solution integrates the Crossing Validation idea to improve the reliability of results. let each fold of train_validation data set become validation dataset once.
  

Firstly, the train_validation dataset will be divided into some folds (I choose 5).  the validation dataset and the training dataset in each iteration will be taken as a list to store in a list variable (nested list). If it has 5 iterations, this variable will have 5 different validation datasets and 5 different training datasets.
```{r}
split_data_CV = function(data,folds){
  num = data%>%nrow()
  num_per_fold = ceiling(num/folds)
  splited_dataset = list()
  for (iter in 1:folds) {
    fold_start = (iter-1)*num_per_fold+1
    fold_end = min(iter*num_per_fold,num)
    fold_indices = seq(fold_start,fold_end)
    splited_dataset[[iter]] = list("train"= data %>% 
                                     filter(!row_number()%in%fold_indices), 
                                   "validation"= data %>%
                                     filter(row_number()%in%fold_indices)) 
  }
  return(splited_dataset)
}
```


Create a function to reduce the amount of training dataset in each iteration and return this decreased training dataset. In addition, for improving sampling randomness further, the seed argument will be the iteration index.
```{r}
train_data_decrease = function(initial_train_data, wanted_train_data_num,fold){
  set.seed(fold)
  initial_data_num = initial_train_data%>% nrow()
  data_indices = sample(1:initial_data_num,wanted_train_data_num, replace = FALSE)
  wanted_train_data = initial_train_data %>% filter(row_number()%in%data_indices)
  return(wanted_train_data)
}
```

Each decreased training dataset and the validation dataset will be poured into the KNN classifier and the accuracy will be calculated. Meanwhile, if K is not declared, it will be 10 by default.
```{r}
library(kknn)
kknn_accuracy = function(ds1,ds2, k_neighbor=10){
  check = kknn(diagnosis~.,train = ds1, test = ds2, k=k_neighbor, distance = 2, kernel ="rectangular")
  fit = fitted(check)
  confusion_matrix = table(ds2$diagnosis, fit)
  acc =  sum(diag(confusion_matrix))/sum(confusion_matrix)
  return(acc)
}

```

In these 5 iterations, every time the train data amount decreases, the accuracy on the decreased training dataset and the validation dataset will be stored. Then take the average of the accuracy of the same amount of training dataset in iterations.    
Finally, plot how the average accuracy on the training dataset and validation dataset changes when the amount of the training dataset changes.
```{r}
##I set that every time the amount of data decreases, I will remove three pieces of data, and the K-fold is 5
folds = 5
inc = 3
all_sample_list = split_data_CV (train_validation_data,folds)
train_total_num = floor(train_validation_data%>%nrow()*(folds-1)/folds)
acc_by_train_ds_num = crossing(sample_list_index = seq(1,5), 
                        train_data_num = seq(15,train_total_num,inc))%>%
  mutate(train = pmap(.l = list(sample_list_index,train_data_num),
                     .f = ~ train_data_decrease(all_sample_list[[.x]][["train"]],
                                                     .y,.x)))%>%
  mutate(validation = map(.x=sample_list_index,.f=~all_sample_list[[.x]][["validation"]]))%>%
  mutate(acc_validation = map2_dbl(.x = train,.y = validation, .f = ~kknn_accuracy(ds1 =.x,ds2=.y)))%>%
  mutate(acc_train = map_dbl(.x = train,.f = ~kknn_accuracy(ds1=.x,ds2=.x)))%>%
  group_by(train_data_num)%>%
  summarize(validation_accuracy = mean(acc_validation), train_accuracy = mean(acc_train))

acc_by_train_ds_num <- acc_by_train_ds_num%>%
  pivot_longer(cols=c(validation_accuracy,train_accuracy),names_to = "Type", values_to = "accuracy")
```
It can be seen from the figure that both the accuracy on the training dataset and the validation dataset surge before the training data increases to about 100, however, the growth rate of both slows down later.
```{r}
ggplot(data=acc_by_train_ds_num,aes(x=train_data_num,y=accuracy,color =Type,linetype= Type))+ geom_smooth() + theme_bw() 
```

## 3.4 Explore how the performance of model varies (both on train data and validation data) when the hyperparameter, k,changes
Similarly, in this part, the idea for crossing validation is still used to get the more reliable accuracy changing situation as the hyperparameter k changes.
```{r}
folds = 5
all_folds_ds = split_data_CV (train_validation_data,folds)
acc_by_hyperparameter = crossing(k = seq(1,100,5),
                                 fold_index = 1:folds)%>%
  mutate(train_per_fold = map(.x=fold_index, .f=~all_folds_ds[[.x]][["train"]]))%>%
  mutate(validation_per_fold = map(.x=fold_index, .f=~all_folds_ds[[.x]][["validation"]]))%>%
  mutate(acc_train = map2_dbl(.x = train_per_fold, .y=k, .f=~kknn_accuracy(ds1=.x,ds2=.x,k_neighbor =.y)))%>%
  mutate(acc_validation = pmap_dbl(.l= list(train_per_fold, validation_per_fold,k),.f=~kknn_accuracy(ds1=..1,ds2=..2,k_neighbor =..3)))%>%
  group_by(k)%>%
  summarize(validation_accuracy = mean(acc_validation), train_accuracy = mean(acc_train))

acc_by_hyperparameter <- acc_by_hyperparameter%>%
  pivot_longer(cols=c(validation_accuracy,train_accuracy),names_to = "Type", values_to = "accuracy")

```
This chart points that, the accuracy on the training dataset decreases continuously as the k-neighbours increases. On the contrary, the accuracy on the validation dataset rises to the peak when the k-neighbours is approximately 25 and then decreases constantly.
```{r}
ggplot(data=acc_by_hyperparameter,aes(x=k,y=accuracy,color =Type,linetype= Type))+geom_smooth() + theme_bw() 

```

## 3.5 Hyperparameter tuning through RandomizedSearchCV and report the performance based on the test data
In KNN, k-neighbours is not only the hyperparameter, but the way to compute the distance and the function to add weights.  
    
    
Generally speaking, Euclidean distance $\sqrt{\sum_{r=1}^n{(x_i-y_i)^2}}$ is selected as the distance metric by default. But there are many other distance measures, like cosine distance, Mahalanobis distance. Each specific dataset has a different optimal distance measurement for classification or regression. The following will only explore the generalization of Euclidean distance, Minkowski distance $\left(\sum_{i=1}^n|x_i-y_i|^p\right)^{1/p}$ as distance metric where p is a hyperparameter.  
  
  
Real-world datasets may has extremely different data volumes for different classes, leading to a situation where the result just belongs to the class with more data volume. This problem especially influences the model performance for datasets with a small distance between classes.That is why weighting the first k neighbours is important. KKNN function provides many kernel functions to add weights,like Gaussian kernel, biweight kernel and epanechnikov kernel, which also can be taken as a hyperparameter.  

So, if the hyperparameter tuning is executed in this situation, the optimal combination of these three hyperparameters needs to be found. But GridsearchCV can not be chosen, because it would need list all combination to validate, which is extremely time-consuming. Compared with GridsearchCV, RandomizedSearchCV does not need to try all possible combinations but sample the random hyperparameter values based on the hyperparameter statistic distribution and then combine them to do the crossing validation.

An external package named RandomSearchR by Lampros will be used (Lampros, 2016). Depending on package "remote", it can be downloaded through command remotes::install_github('mlampros/RandomSearchR')
```{r, results='hide', warning=FALSE}
library(RandomSearchR)
## list all choices of hyperparameter
grid_kknn = list(k = 1:50, 
                 distance = 1:5,
                 kernel = c("rectangular", "triangular", "epanechnikov", "biweight","cos", "inv", "gaussian", "rank", "optimal"))

## make diagnosis results become numerical value (1,2)
diagnosis_vec = train_validation_data[,1]
diagnosis_vec = c(1:length(unique(diagnosis_vec)))[match(diagnosis_vec, sort(unique(diagnosis_vec)))]
data_vec = train_validation_data[,-1]
## create a formula for the following random_search_resample function
form <- as.formula(paste('diagnosis ~', paste(names(data_vec),collapse = '+')))

ALL_DATA = train_validation_data
## execute the RandomizedSearchCV of which the number of randomly sampling hyperparameter combinations (iteration) is 30 and K-folds for crossing validation is 5.
res_knn = random_search_resample( as.factor(diagnosis_vec),
                                  tune_iters = 50,
                                  resampling_method = list(method = 'cross_validation',
                                                           repeats=NULL, 
                                                           sample_rate = NULL,
                                                           folds = 5),
                                  ALGORITHM = list(package = require(kknn), algorithm = kknn),
                                  grid_params = grid_kknn,
                                  DATA = list(formula = form, train = ALL_DATA),
                                  Args = NULL,
                                  regression = FALSE,
                                  re_run_params=FALSE)
```
```{r}
##computing accuracy
acc = function(y_true, preds) { 
  out = table(y_true, max.col(preds, ties.method = "random"))
  acc = sum(diag(out))/sum(out)
  acc
}

##Obtaining the performance evaluation result of this KNN classifier
perf = performance_measures(list_objects = list(kknn = res_knn),
                            eval_metric = acc,
                            sort = list(variable = 'Mean', decreasing = TRUE))
accuracy_validation_table = dplyr:: select(perf$test_params$kknn,k,distance,kernel,accuracy=Mean) 
accuracy_train_table = dplyr:: select(perf$train_params$kknn, k,distance,kernel,accuracy=Mean)
head(accuracy_validation_table,10)

```


Through RandomizedSearchCV, a accuracy rank list can be obtained. So, the optimal hyperparameters should be k=11, p_distance (Minkowski) = 3, kernel function="epanechnikov".

For reporting the classifier performance in more detail, the package named "caret" will be utilized of which the function "confusionMatrix()" can provides many metric results. Notably, Sensitivity is 0.9767 and Specificity is 0.8246, which means the ability of this classifier to predict the tumour is malignant (0) is far better than the ability to predict that the tumour is benign (1).
```{r}
library(caret)
test_knn = kknn(diagnosis~., train = train_validation_data, test = test_data, k = 11, distance = 3, kernel = "epanechnikov")
truth = test_data$diagnosis
pred = fitted(test_knn)
confusionMatrix(table(pred, truth))
```

## 3.6 The introduction of kd tree and the efficiency and performance comparision
KNN is the representative for "Lazy Learning".  It means that some algorithms do not need computing model parameters in data training, and even doesn't need data training. However, to some degree, these algorithms just delay the time on data training to the data test. KNN needs to utilize all training data in the test stage. Based on the time complexity in the test stage, O(N*D), where N is the number of a sample set, and D is the number of dimensions, a conclusion can be made that if N or D becomes enormous, the time complexity and computational volume becomes large. It also means KNN are not good at to deal with a very high dimensions dataset.

KD tree is a kind of tree data structure that stores instance points in multidimensional space for rapid retrieval. If the dataset for training is stored in the KD tree in advance, part of the time complexity of KNN can be transferred to the space complexity, and the efficiency can be improved in the data test stage.  
  
The following will apply for "RANN" package written by Jefferis, a wrapper for Approximate Nearest Neighbours (ANN) C++ library, where nn2 function uses the KD tree to store training datasets to search the p number of near neighbours for each point in a test dataset (Jefferis, 2019).

Because the amount of breast cancer data is too small, in order to enlarge the time consumption comparison between KNN search with KD tree and traditional KNN search, these two methods are repeated to execute a thousand times to repeat 1000 times.
```{r}
library(RANN)
knn_on_kdtree = system.time(lapply(1:1000,function(x) test1 <- nn2(train_validation_data[-1], k=11, query = test_data[-1])))[3]
normal_knn = system.time(lapply(1:1000,function(x) test2 <-  kknn(diagnosis~., train_validation_data, test_data, k = 11)))[3]
```
From the figures, it can be seen that the time elapsed on KNN search on a KD tree is less than traditional KNN search 
```{r}
time_table = data.frame(Type = c("KNN search on a KD Tree","Traditional KNN"), Time = c(as.numeric(knn_on_kdtree),as.numeric(normal_knn)))
time_table
ggplot(data = time_table, aes(x=Type,y=Time,fill=Type))+ 
  geom_bar(stat="identity",width=0.5)+
  labs(title = "The comparision about Time Elapsed", y="Time in Seconds") + 
  theme(plot.title = element_text(hjust = 0.5))+ 
  theme_bw() 
```


```{r}
compute_mode =  function(vec){
  temp = table(vec)
  nums_of_mode <- max(temp)
  mode = as.numeric(names(temp))[temp == nums_of_mode]
  return(mode)
}

kdtree_knn = function(train,test){
  neighbours_list =nn2(train[-1], k=11, query = test[-1])[["nn.idx"]]
  row_num = neighbours_list%>%nrow()
  preds = c()
  for(x in 1:row_num){
    index = neighbours_list[x,]
    neighbours = train%>%filter(row_number()%in%index)%>%select(diagnosis)
    preds = c(preds,compute_mode(neighbours))
  }
  return(preds)
}

traditional_knn = function(train,test){
  check_knn = kknn(diagnosis~., train, test, k = 11)
  pred_vector = fitted(check_knn)
  return(pred_vector)
}
```
In terms of performance comparison, the gap between these methods is not very obvious.
```{r}
preds_kdtree = kdtree_knn(train_validation_data,test_data)
preds_tradition = traditional_knn(train_validation_data,test_data)
truth = test_data$diagnosis
kd_preds_table = table(truth,preds_kdtree)
confusionMatrix(kd_preds_table)
trad_preds_table = table(truth,preds_tradition)
confusionMatrix(trad_preds_table)
```

## 3.7 Further Consideration
All packages about KD tree search just apply for the basic KNN to return the near neighbours. They just only take K as a hyperparameter but ignore the distance measures and the method about weighting, unlike the KKNN package. But this phenomenon very obviously means that k as the hyperparameter in KNN is much more important than the distance measures and the method about weighting. But in other algorithms where it is difficult to intuitively understand the importance of all different hyperparameters, how to evaluate their importance from a mathematical and theoretical perspective may be a direction that deserves to study.

## 3.8 References
[1] W.H.Wolberg, W.N.Street,O.L.Mangasarian (1995). Breast Cancer Wisconsin (Diagnostic) Data Set [online]. Acessed: 18 December 2021. Available: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29  
  
[2] Mouselimis Lampros (2016).Random search and resampling techniques in R [online]. Acessed: 19 December 2021. Available: http://mlampros.github.io/2016/03/14/random_search_R/  
  
[3] Gregory Jefferis (2019). RAAN [online]. Acessed: 20 December 2021. Available: https://www.rdocumentation.org/packages/RANN/versions/2.6.1  







